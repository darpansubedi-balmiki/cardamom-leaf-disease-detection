# Cardamom Leaf Disease Detection System

A complete full-stack and mobile application system for detecting diseases in cardamom leaves using deep learning. The system classifies leaf images into three categories:
- **Colletotrichum Blight** (à¤•à¥‹à¤²à¥‡à¤Ÿà¥‹à¤Ÿà¥à¤°à¤¿à¤•à¤® à¤¬à¥à¤²à¤¾à¤‡à¤Ÿ)
- **Phyllosticta Leaf Spot** (à¤«à¤¾à¤‡à¤²à¥‹à¤¸à¥à¤Ÿà¤¿à¤•à¥à¤Ÿà¤¾ à¤ªà¤¾à¤¤ à¤¦à¤¾à¤—)
- **Healthy** (à¤¸à¥à¤µà¤¸à¥à¤¥)

## ğŸ”„ Just Pulled Changes?

**â†’ Read [AFTER_PULL.md](AFTER_PULL.md)** - Complete guide for what to do after pulling updates!

Quick summary: Install dependencies â†’ Verify installation â†’ Choose your path (train model or start using system)

---

## ğŸ‰ READY TO TRAIN YOUR MODEL?

**If you have collected cardamom disease images:**

### ğŸ“š Quick Start Guides:
1. **[YOURE_READY.md](YOURE_READY.md)** - Visual guide showing you're ready! ğŸ¯
2. **[START_TRAINING_NOW.md](START_TRAINING_NOW.md)** - 3-step quick start (2 min read) âš¡
3. **[TRAINING_YOUR_MODEL.md](TRAINING_YOUR_MODEL.md)** - Complete detailed guide ğŸ“–

### âš¡ Super Quick Start:
```bash
cd backend
python train.py
```

### Current System Status:
- âš ï¸ **Model Status**: Untrained (using random weights)
- âš ï¸ **Predictions**: Low accuracy (~35% confidence)
- âœ… **After Training**: 90%+ confidence with accurate predictions!

**With your dataset ready, training takes 30-60 minutes (GPU) to get a production-ready model!**

## ğŸŒŸ Features

- **Deep Learning Classification**: PyTorch-based EfficientNetV2 model for disease detection
- **Grad-CAM Visualization**: Visual explanation showing which leaf regions influenced predictions
- **Background Removal**: U2-Net integration (placeholder ready)
- **Modern Web Interface**: React TypeScript frontend with responsive design
- **ğŸ“± Mobile App**: React Native app with camera integration and Nepali language support
- **Real-time Predictions**: Fast inference with confidence scores
- **RESTful API**: FastAPI backend with automatic documentation
- **Bilingual Support**: English and Nepali (à¤¨à¥‡à¤ªà¤¾à¤²à¥€) interface

## ğŸ—ï¸ Architecture

### Backend (FastAPI + PyTorch)
- **Framework**: FastAPI with CORS support
- **Model**: EfficientNetV2-S classifier (3 output classes) - **requires training**
- **Preprocessing**: ImageNet normalization, 224x224 resizing
- **Visualization**: Grad-CAM heatmap generation
- **Background Removal**: U2-Net placeholder (ready for integration)

### Frontend (React + TypeScript + Vite)
- **Framework**: React 19 with TypeScript
- **Build Tool**: Vite for fast development
- **HTTP Client**: Axios for API communication
- **UI**: Modern gradient design, responsive layout
- **Features**: Image upload, preview, results display, error handling

### Mobile App (React Native + Expo)
- **Framework**: React Native with Expo
- **Language**: TypeScript
- **Features**: Camera capture, gallery picker, disease detection
- **UI**: Bilingual (English/Nepali), native mobile experience
- **Navigation**: React Navigation
- **Comprehensive Disease Info**: Full information in Nepali for farmers

## ğŸ“ Project Structure

```
cardamom-leaf-disease-detection/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ main.py              # FastAPI entry point
â”‚   â”‚   â”œâ”€â”€ schemas.py           # Pydantic models
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ cardamom_model.py      # CNN classifier
â”‚   â”‚   â”‚   â””â”€â”€ u2net_segmenter.py     # Background removal
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ image_preprocess.py    # Image preprocessing
â”‚   â”‚       â”œâ”€â”€ grad_cam.py            # Grad-CAM implementation
â”‚   â”‚       â””â”€â”€ overlay.py             # Heatmap overlay
â”‚   â”œâ”€â”€ models/                  # Trained model weights (gitignored)
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ main.tsx            # React entry
â”‚   â”‚   â”œâ”€â”€ App.tsx             # Main component
â”‚   â”‚   â”œâ”€â”€ App.css             # Styles
â”‚   â”‚   â”œâ”€â”€ index.css           # Global styles
â”‚   â”‚   â””â”€â”€ api/
â”‚   â”‚       â””â”€â”€ client.ts       # API client
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tsconfig.json
â”‚   â””â”€â”€ vite.config.ts
â”‚
â”œâ”€â”€ cardamom-mobile-app/         # ğŸ“± React Native Mobile App
â”‚   â”œâ”€â”€ App.tsx                  # Main app with navigation
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ screens/             # HomeScreen, ResultScreen, DiseaseInfoScreen
â”‚   â”‚   â”œâ”€â”€ components/          # Reusable UI components
â”‚   â”‚   â”œâ”€â”€ services/            # API client
â”‚   â”‚   â”œâ”€â”€ data/                # Disease info in Nepali
â”‚   â”‚   â”œâ”€â”€ types/               # TypeScript types
â”‚   â”‚   â””â”€â”€ utils/               # Helper functions
â”‚   â”œâ”€â”€ app.json                 # Expo configuration
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ README.md
â”‚
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## ğŸš€ Getting Started

### Prerequisites

- **Backend**:
  - **Python 3.9 - 3.13** (Recommended: Python 3.11 or 3.12)
  - pip (latest version)
  - **Having issues?** See [PYTHON_VERSION_GUIDE.md](PYTHON_VERSION_GUIDE.md)

- **Frontend**:
  - Node.js 18 or higher
  - npm or yarn

- **Mobile App**:
  - Node.js 18 or higher
  - Expo CLI (`npm install -g expo-cli`)
  - Expo Go app (for testing on real devices)

### Installation

#### 1. Backend Setup

```bash
# Navigate to backend directory
cd backend

# Create virtual environment
python -m venv venv

# Activate virtual environment
# On Linux/Mac:
source venv/bin/activate
# On Windows:
venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

#### 2. Frontend Setup

```bash
# Navigate to frontend directory
cd frontend

# Install dependencies
npm install
```

#### 3. Mobile App Setup

```bash
# Navigate to mobile app directory
cd cardamom-mobile-app

# Install dependencies
npm install
```

### Running the Application

#### Start Backend Server

```bash
cd backend
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

The backend API will be available at:
- **API**: http://localhost:8000
- **Interactive Docs**: http://localhost:8000/docs
- **ReDoc**: http://localhost:8000/redoc

#### Start Frontend Development Server

```bash
cd frontend
npm run dev
```

The frontend will be available at: http://localhost:5173

#### Start Mobile App

```bash
cd cardamom-mobile-app
npm start
```

Then:
- Scan the QR code with Expo Go app on your device
- Or press `i` for iOS simulator, `a` for Android emulator, `w` for web

**Note**: To connect the mobile app to your backend:
1. Find your computer's local IP address
2. Update `src/services/api.ts` in the mobile app:
   ```typescript
   const API_BASE_URL = 'http://YOUR_LOCAL_IP:8000';
   ```

## ğŸ“¡ API Endpoints

### GET /health
Health check endpoint.

**Response:**
```json
{
  "status": "ok"
}
```

### POST /predict
Upload an image for disease prediction.

**Request:**
- Content-Type: `multipart/form-data`
- Body: `file` (image file)

**Response:**
```json
{
  "class_name": "Colletotrichum Blight",
  "confidence": 0.85,
  "heatmap": "base64_encoded_png_string"
}
```

## ğŸ¯ How It Works

1. **Upload**: User selects a cardamom leaf image
2. **Preprocessing**:
   - Background removal (U2-Net - placeholder)
   - Resize to 224x224
   - Normalize with ImageNet statistics
3. **Inference**: CNN model predicts disease class
4. **Visualization**: Grad-CAM generates heatmap showing important regions
5. **Results**: Display class name, confidence, and heatmap overlay

## ğŸ”§ Model Integration

### Current State (Placeholder)

The system currently uses placeholder models for demonstration:
- **Classifier**: Simple 4-layer CNN with random weights
- **U2-Net**: Passes images through unchanged

### Integrating Trained Models

When you have trained models:

1. **Place Model Weights**:
   - Save trained classifier as `backend/models/cardamom_model.pt`
   - Save U2-Net as `backend/models/u2net.pth`

2. **Update Model Architecture**:
   - Modify `backend/app/models/cardamom_model.py` to match your architecture (e.g., EfficientNetV2)
   - Implement U2-Net loading in `backend/app/models/u2net_segmenter.py`

3. **Adjust Preprocessing** (if needed):
   - Update `backend/app/utils/image_preprocess.py`

The system will automatically detect and load trained weights if they exist in the models directory.

## ğŸ§ª Testing

### Backend Testing

```bash
cd backend
pytest
```

### Frontend Testing

```bash
cd frontend
npm run lint
npm run build
```

### Manual Testing

1. Start both servers
2. Open http://localhost:5173
3. Upload a test image
4. Verify prediction results and heatmap display

## ğŸ¨ UI Features

- **Modern Design**: Gradient background (purple/blue)
- **Responsive**: Works on desktop and mobile
- **Interactive**: Hover effects, loading states
- **Informative**: Error messages, confidence visualization
- **Visual Explanation**: Grad-CAM heatmap overlay

## ğŸ” Security Notes

- File type validation on frontend and backend
- CORS configured for specific origins
- Request timeouts to prevent hanging
- Input sanitization and error handling

## ğŸ“ Development Notes

### Backend
- Uses PyTorch hooks for Grad-CAM
- FastAPI automatic API documentation
- Proper error handling and logging
- Device auto-detection (CUDA/CPU)

### Frontend
- TypeScript for type safety
- Axios with 30s timeout
- Memory cleanup (URL.revokeObjectURL)
- Error boundary handling

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## ğŸ“„ License

MIT

## ğŸ™ Acknowledgments

- PyTorch team for the deep learning framework
- FastAPI for the excellent web framework
- React team for the UI library
- U2-Net authors for background removal architecture
